# Assignment 2

The goals of this assignment are as follows:

- understand **Neural Networks** and how they are arranged in layered architectures
- understand and be able to implement (vectorized) **backpropagation**
- implement various **update rules** used to optimize Neural Networks
- implement **Batch Normalization** and **Layer Normalization** for training deep networks
- implement **Dropout** to regularize networks
- understand the architecture of **Convolutional Neural Networks** and get practice with training these models on data
- gain experience with a major deep learning framework, such as **TensorFlow** or **PyTorch**.


## codes

### Q1: Fully-connected Neural Network (20 points)

The IPython notebook [FullyConnectedNets.ipynb](./FullyConnectedNets.ipynb) will introduce us to our modular layer design, and then use those layers to implement fully-connected networks of arbitrary depth. To optimize these models we will implement several popular update rules.

### Q2: Batch Normalization (30 points)

In the IPython notebook [BatchNormalization.ipynb](./BatchNormalization.ipynb) us will implement batch normalization, and use it to train deep fully-connected networks.

### Q3: Dropout (10 points)

The IPython notebook [Dropout.ipynb](./Dropout.ipynb) will help us implement Dropout and explore its effects on model generalization.

### Q4: Convolutional Networks (30 points)

In the IPython Notebook [ConvolutionalNetworks.ipynb](./ConvolutionalNetworks.ipynb) we will implement several new layers that are commonly used in convolutional networks.

### Q5: PyTorch / TensorFlow on CIFAR-10 (10 points)

For this last part, we will be working in TensorFlow in the IPython Notebook [TensorFlow.ipynb](./TensorFlow.ipynb).

### Todos
 - Write what I learnt about setting this assignment (takeaway)
 - Complete Q3 to Q5 using PyTorch.
 - ...